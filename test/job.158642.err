The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [01:31<03:03, 91.71s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:32<03:05, 92.96s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:47<03:34, 107.04s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [01:47<03:34, 107.03s/it]
[rank0]:[W1026 11:14:28.149244867 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
