The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:54<00:54, 54.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:55<00:55, 55.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 41.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 41.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.78s/it]
[rank0]:[W1025 10:06:30.608206700 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
