The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.98s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.00s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:25, 12.71s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:25, 12.60s/it]

[rank0]:[W1025 09:24:33.463625963 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
