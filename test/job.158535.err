The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.32s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.32s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.06s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.08s/it]
[rank0]:[W1025 10:09:08.235237913 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
