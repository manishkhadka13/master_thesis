Starting job at Sat Oct 25 09:24:06 CEST 2025
Job ID: 158539
Running on node: ailab-l4-02
Active conda environment: llm
Python path: /ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/bin/python
Python version: Python 3.10.18
Loaded 1560 rows from custom_dataset.csv
Processing all 1560 samples...
Error loading model: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 22.03 GiB of which 94.00 MiB is free. Including non-PyTorch memory, this process has 10.60 GiB memory in use. Process 110892 has 11.33 GiB memory in use. Of the allocated memory 10.37 GiB is allocated by PyTorch, and 51.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Job completed at Sat Oct 25 09:24:35 CEST 2025
