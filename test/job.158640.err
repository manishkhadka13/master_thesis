The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:47<01:47, 107.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:48<01:48, 108.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:38<00:00, 73.81s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:38<00:00, 79.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:38<00:00, 74.42s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:38<00:00, 79.42s/it]
[rank0]:[W1026 12:15:15.128382382 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
