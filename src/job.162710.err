The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 407, in hf_raise_for_status
[rank0]:     response.raise_for_status()
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
[rank0]:     raise HTTPError(http_error_msg, response=self)
[rank0]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-Guard-3/resolve/main/tokenizer_config.json

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 479, in cached_files
[rank0]:     hf_hub_download(
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1010, in hf_hub_download
[rank0]:     return _hf_hub_download_to_cache_dir(
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1117, in _hf_hub_download_to_cache_dir
[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1658, in _raise_on_head_call_error
[rank0]:     raise head_call_error
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1546, in _get_metadata_or_catch_error
[rank0]:     metadata = get_hf_file_metadata(
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1463, in get_hf_file_metadata
[rank0]:     r = _request_wrapper(
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
[rank0]:     response = _request_wrapper(
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
[rank0]:     hf_raise_for_status(response)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 457, in hf_raise_for_status
[rank0]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank0]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-69046ea5-0f47069b554c61af115f60bb;e7638470-52c4-4708-a596-1024691f0c33)

[rank0]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-Guard-3/resolve/main/tokenizer_config.json.
[rank0]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank0]: If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/src/llm_judge.py", line 19, in <module>
[rank0]:     tokenizer = AutoTokenizer.from_pretrained(model_name)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1073, in from_pretrained
[rank0]:     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 905, in get_tokenizer_config
[rank0]:     resolved_config_file = cached_file(
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 322, in cached_file
[rank0]:     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 511, in cached_files
[rank0]:     raise OSError(
[rank0]: OSError: meta-llama/Llama-Guard-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank0]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
[rank1]: Traceback (most recent call last):
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 407, in hf_raise_for_status
[rank1]:     response.raise_for_status()
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
[rank1]:     raise HTTPError(http_error_msg, response=self)
[rank1]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-Guard-3/resolve/main/tokenizer_config.json

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 479, in cached_files
[rank1]:     hf_hub_download(
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1010, in hf_hub_download
[rank1]:     return _hf_hub_download_to_cache_dir(
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1117, in _hf_hub_download_to_cache_dir
[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1658, in _raise_on_head_call_error
[rank1]:     raise head_call_error
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1546, in _get_metadata_or_catch_error
[rank1]:     metadata = get_hf_file_metadata(
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1463, in get_hf_file_metadata
[rank1]:     r = _request_wrapper(
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
[rank1]:     response = _request_wrapper(
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
[rank1]:     hf_raise_for_status(response)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 457, in hf_raise_for_status
[rank1]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank1]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-69046ea5-1dc7948b2da655d13f3800aa;eac2f1f0-27ec-4242-96a8-86cb878f4fde)

[rank1]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-Guard-3/resolve/main/tokenizer_config.json.
[rank1]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank1]: If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/src/llm_judge.py", line 19, in <module>
[rank1]:     tokenizer = AutoTokenizer.from_pretrained(model_name)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1073, in from_pretrained
[rank1]:     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 905, in get_tokenizer_config
[rank1]:     resolved_config_file = cached_file(
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 322, in cached_file
[rank1]:     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[rank1]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 511, in cached_files
[rank1]:     raise OSError(
[rank1]: OSError: meta-llama/Llama-Guard-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank1]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
[rank0]:[W1031 09:09:09.353556528 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1031 09:09:10.294000 2621852 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2623413 closing signal SIGTERM
E1031 09:09:10.409000 2621852 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2623412) of binary: /ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/bin/python3.10
Traceback (most recent call last):
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
llm_judge.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-31_09:09:10
  host      : ailab-l4-11.srv.aau.dk
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2623412)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
