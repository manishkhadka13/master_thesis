The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.77s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.29s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:19<00:45, 45.18s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:23<00:47, 47.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:34<00:00, 31.20s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:34<00:00, 31.33s/it]
Some parameters are on the meta device because they were offloaded to the cpu.
  0%|          | 0/1560 [00:00<?, ?it/s]  0%|          | 0/1560 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 3812, in get_loc
[rank0]:     return self._engine.get_loc(casted_key)
[rank0]:   File "pandas/_libs/index.pyx", line 167, in pandas._libs.index.IndexEngine.get_loc
[rank0]:   File "pandas/_libs/index.pyx", line 196, in pandas._libs.index.IndexEngine.get_loc
[rank0]:   File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
[rank0]:   File "pandas/_libs/hashtable_class_helper.pxi", line 7096, in pandas._libs.hashtable.PyObjectHashTable.get_item
[rank0]: KeyError: 'variant'

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/src/llm_judge.py", line 41, in <module>
[rank0]:     judge_input = make_judge_prompt(row['variant'], row['model_response'])
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/series.py", line 1133, in __getitem__
[rank0]:     return self._get_value(key)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/series.py", line 1249, in _get_value
[rank0]:     loc = self.index.get_loc(label)
[rank0]:   File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 3819, in get_loc
[rank0]:     raise KeyError(key) from err
[rank0]: KeyError: 'variant'
[rank0]:[W1026 16:46:16.440789144 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1026 16:46:17.246000 3588502 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3588516 closing signal SIGTERM
E1026 16:46:17.762000 3588502 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 3588515) of binary: /ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/bin/python3.10
Traceback (most recent call last):
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ceph/home/student.aau.dk/jx88vo/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
llm_judge.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_16:46:17
  host      : ailab-l4-02.srv.aau.dk
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3588515)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
